{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'band_singer': 'The Guess Who', 'ranking': '3', 'title': '\"American Woman\"', 'url': '/wiki/The_Guess_Who'}, {'band_singer': 'B.J. Thomas', 'ranking': '4', 'title': '\"Raindrops Keep Fallin\\' on My Head\"', 'url': '/wiki/B.J._Thomas'}]\n"
     ]
    }
   ],
   "source": [
    "########\n",
    "# The example below illustrates scraping a table from a Wikipedia page\n",
    "########\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "wiki_url = \"http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1970\"\n",
    "web_page = requests.get(wiki_url)\n",
    "\n",
    "soup = BeautifulSoup(web_page.content, 'lxml')\n",
    "table_classes = {\"class\": [\"sortable\", \"plainrowheaders\"]}\n",
    "wikitables = soup.findAll(\"table\", table_classes)\n",
    "\n",
    "tables = soup.findAll(\"table\", { \"class\" : \"wikitable\" })\n",
    "table = tables[0]\n",
    "\n",
    "songs = []\n",
    "rows = table.find_all('tr')\n",
    "for row in rows[1:101]:\n",
    "    cols = row.find_all('td')\n",
    "    cols = [ele.text.strip() for ele in cols]    \n",
    "    url = row.find_all('a')[len(row.find_all('a'))-1].get('href')  \n",
    "    songs.append(dict([('band_singer', cols[2]), ('ranking', cols[0]), ('title', cols[1]), ('url', url)]))\n",
    "print(songs[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1992\n"
     ]
    }
   ],
   "source": [
    "yearstext = {}\n",
    "for yr in range(1992, 1993):\n",
    "    wiki_url = \"http://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_\" + str(yr)\n",
    "    print(wiki_url)\n",
    "    web_page = requests.get(wiki_url)\n",
    "    time.sleep(1)\n",
    "    yearstext['%d'%yr]= web_page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########\n",
    "# The example below illustrates cleaning of data\n",
    "########\n",
    "\n",
    "\"\"\"\n",
    "Function\n",
    "--------\n",
    "parse_year\n",
    "\n",
    "Inputs\n",
    "------\n",
    "the_year: the year you want the singles for\n",
    "yeartext_dict: a dictionary with keys as integer years and values the downloaded web pages \n",
    "    from wikipedia for that year.\n",
    "   \n",
    "Returns\n",
    "-------\n",
    "\n",
    "a list of dictionaries, each of which corresponds to a single and has the\n",
    "following data:\n",
    "\n",
    "Eg:\n",
    "\n",
    "{'band_singer': ['Brandy', 'Monica'],\n",
    "  'ranking': 2,\n",
    "  'song': ['The Boy Is Mine'],\n",
    "  'songurl': ['/wiki/The_Boy_Is_Mine_(song)'],\n",
    "  'titletext': '\" The Boy Is Mine \"',\n",
    "  'url': ['/wiki/Brandy_Norwood', '/wiki/Monica_(entertainer)']}\n",
    "  \n",
    "A dictionary with the following data:\n",
    "    band_singer: a list of bands/singers who made this single\n",
    "    song: a list of the titles of songs on this single\n",
    "    songurl: a list of the same size as song which has urls for the songs on the single \n",
    "        (see point 3 above)\n",
    "    ranking: ranking of the single\n",
    "    titletext: the contents of the table cell\n",
    "    band_singer: a list of bands or singers on this single\n",
    "    url: a list of wikipedia singer/band urls on this single: only put in the part \n",
    "        of the url from /wiki onwards\n",
    "    \n",
    "\n",
    "Notes\n",
    "-----\n",
    "See description and example above.\n",
    "\"\"\"\n",
    "import re\n",
    "def parse_year(the_year, yeartext_dict):    \n",
    "    web_page_content = yeartext_dict['%s' %str(the_year)]\n",
    "    soup = BeautifulSoup(web_page_content, 'lxml')\n",
    "    table_classes = {\"class\": [\"sortable\", \"plainrowheaders\"]}\n",
    "    wikitables = soup.findAll(\"table\", table_classes)\n",
    "\n",
    "    tables = soup.findAll(\"table\", { \"class\" : \"wikitable\" })\n",
    "    table = tables[0]\n",
    "    \n",
    "    stat = []\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:101]:\n",
    "        songs = []\n",
    "        songurls = []\n",
    "        singers = []\n",
    "        singerurls = []\n",
    "        cols = row.find_all(['td', 'th'])\n",
    "        rank = cols[0].text.strip()\n",
    "        titletext = cols[1].text.strip()\n",
    "        urls = cols[1].find_all('a')\n",
    "        \n",
    "        '''\n",
    "        ### Parse song names and song urls ###\n",
    "        ### Need to be more precise ###\n",
    "        '''\n",
    "        if len(urls) == 0 and titletext.find('/') == -1:\n",
    "            songs.append(titletext)\n",
    "            songurls.append(None)\n",
    "            #print('if_1')\n",
    "        elif len(urls) == 1 and titletext.find('/') == -1:\n",
    "            songs.append(titletext)\n",
    "            songurls.append(urls[0].get('href'))\n",
    "            #print('if_2')\n",
    "        elif len(urls) == 2 and titletext.find('/') != -1:\n",
    "            splited_titletext = titletext.split(' / ')\n",
    "            for word in splited_titletext:\n",
    "                word = word.strip('\"')\n",
    "                songs.append(word)\n",
    "            for url in urls:                \n",
    "                songurls.append(url.get('href'))\n",
    "        else:\n",
    "            songs.append(titletext)\n",
    "            songurls.append(urls[0].get('href'))\n",
    "        singertext = cols[2].text.strip()\n",
    "        urls = cols[2].find_all('a')\n",
    "        textchecklist = [\"featuring\", \"and\", ',']\n",
    "        \n",
    "        '''\n",
    "        ### Parse singers' name and singers' urls ###\n",
    "        ### Might need more elaborations ###\n",
    "        '''\n",
    "        if len(urls) == 1 :    \n",
    "            singers.append(singertext)\n",
    "            singerurls.append(urls[0].get('href'))\n",
    "            #print('if_1')\n",
    "        elif len(urls) > 1 and any(word in singertext for word in textchecklist):\n",
    "            #splited_text = re.split('featuring |\\sand\\s |,', singertext);\n",
    "            splited_text = re.split('featuring | and |,', singertext);\n",
    "            if len(urls) == len(splited_text):\n",
    "                clean_splited_text = []\n",
    "                for ele in splited_text:\n",
    "                    clean_splited_text.append(ele.strip())\n",
    "                singers = clean_splited_text\n",
    "                for url in urls:                \n",
    "                    singerurls.append(url.get('href'))\n",
    "            else:\n",
    "                clean_splited_text = []\n",
    "                for ele in splited_text:\n",
    "                    clean_splited_text.append(ele.strip())\n",
    "                singers = clean_splited_text[0:len(urls)]\n",
    "                for url in urls:                \n",
    "                    singerurls.append(url.get('href'))\n",
    "        else:\n",
    "            singers.append(singertext)\n",
    "            singerurls.append(None)\n",
    "        stat.append(dict([('band_singer', singers), ('ranking', int(rank)), ('song', songs), \\\n",
    "                          ('songurl', songurls), ('titletext', titletext), ('url', singerurls)]))\n",
    "    return stat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'band_singer': ['Boyz II Men'], 'ranking': 1, 'song': ['\"End of the Road\"'], 'songurl': ['/wiki/End_of_the_Road'], 'titletext': '\"End of the Road\"', 'url': ['/wiki/Boyz_II_Men']}, {'band_singer': ['Sir Mix-a-Lot'], 'ranking': 2, 'song': ['\"Baby Got Back\"'], 'songurl': ['/wiki/Baby_Got_Back'], 'titletext': '\"Baby Got Back\"', 'url': ['/wiki/Sir_Mix-a-Lot']}, {'band_singer': ['Kris Kross'], 'ranking': 3, 'song': ['\"Jump\"'], 'songurl': ['/wiki/Jump_(Kris_Kross_song)'], 'titletext': '\"Jump\"', 'url': ['/wiki/Kris_Kross']}, {'band_singer': ['Vanessa Williams'], 'ranking': 4, 'song': ['\"Save the Best for Last\"'], 'songurl': ['/wiki/Save_the_Best_for_Last'], 'titletext': '\"Save the Best for Last\"', 'url': ['/wiki/Vanessa_L._Williams']}, {'band_singer': ['TLC'], 'ranking': 5, 'song': ['\"Baby-Baby-Baby\"'], 'songurl': ['/wiki/Baby-Baby-Baby'], 'titletext': '\"Baby-Baby-Baby\"', 'url': ['/wiki/TLC_(band)']}]\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "print(parse_year(1992, yearstext)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
